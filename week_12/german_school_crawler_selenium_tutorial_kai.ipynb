{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import math\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options # adjust Chrome window size\n",
    "import time, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OverviewTabKeys = ['Language of instruction', 'Standard length of studies', 'Degree', 'Area of Focus', 'Tuition fees']\n",
    "AdmissionTabKeys = ['Admission requirements (Germany)','Admission requirements (Link)','Admission Mode', 'Admission Semester', 'Lecture Period', 'Website']\n",
    "ContactTabKeys = ['International Office (AAA)', 'AAA Mail', 'AAA Link']\n",
    "#HrefOnlyKeys = ['Admission requirements (Link)', 'Website']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LoadBrowser( string ):\n",
    "    \"\"\"\n",
    "    enter a string and return the specified driver of corresponding browser.\n",
    "    \"\"\"\n",
    "    ChromeList = ['chrome', 'Chrome']\n",
    "    FireFoxList = ['firefox', 'FireFox', 'Firefox']\n",
    "    IEList = ['ie', 'IE', 'Ie']\n",
    "    driver = None\n",
    "    if string in ChromeList:\n",
    "        driver = webdriver.Chrome()\n",
    "    elif string in FireFoxList:\n",
    "        driver = webdriver.Firefox()\n",
    "    elif string in IEList:\n",
    "        driver = webdriver.Ie()\n",
    "    else:\n",
    "        raise ValueError( string )\n",
    "    return driver\n",
    "\n",
    "def Crawler(browser, web_addr, sleep=1, timeout=30):\n",
    "    if timeout >0: browser.set_page_load_timeout(timeout)\n",
    "    # Katolon Behavior\n",
    "    try: browser.get( web_addr ) # open web by selenium webdriver\n",
    "    except TimeoutException:\n",
    "        print('time out after %d when loading page, stop loading and proceed to next operations' % timeout)\n",
    "        browser.execute_script('window.stop()')\n",
    "    time.sleep(sleep)\n",
    "    return browser.page_source\n",
    "\n",
    "# helper function: bs4 parser will mistakenly add semicolon after ampersand(&)'s refentity,\n",
    "# resulting in invalid web address format\n",
    "def RefAmpersand(final_soup_href):\n",
    "    \"\"\"\n",
    "    sol method is to first parser codecs into BeautifulSoup using html parser, so '&' would be '&amp;' in original web address.\n",
    "    Then the function replace '&amp;' with '&' (tranform back) to make web address still valid.\n",
    "    \"\"\"\n",
    "    if final_soup_href != None:\n",
    "        final_soup_href = re.sub( r'\\&amp;', r'&', final_soup_href)\n",
    "    return final_soup_href\n",
    "\n",
    "def Parser(soup, task, computePages=True, ref_ampersand=True):\n",
    "    \"\"\"\n",
    "    soup: source data of type BS4.soup\n",
    "    task(return Error if not the following three values):\n",
    "        crawlResultsFound: return total results (and total pages) that needs crawled.\n",
    "            Also return the string \"OOO results found\".\n",
    "        crawlSchoolList: get program title and parser it into 4 information: program link, \n",
    "            program name, name of school, school location, stored and returned by pd.DataFrame\n",
    "        crawlProgrammDetail: get all information of the target program, specified in OverviewTabKeys, \n",
    "            AdmissionTabKeys and ContactTabKeys.\n",
    "    computePages(bool): affect task 'crawlResultsFound'. Additionally return total pages if True.\n",
    "    ref_ampersand(bool): affect all href, modifying the illegal '&amp;' to be '&' as \n",
    "        long as it appears in href.\n",
    "    \"\"\"\n",
    "    if task == 'crawlResultsFound':\n",
    "        targetblock = soup.find( 'div', attrs = {'class':'view-tab', 'id':'course-list'} ) \n",
    "        targetblock = targetblock.parent.find('p', attrs={'class':'count-mobile count-mobile-m'})\n",
    "        sentence = \"\"\n",
    "        for idx, string in enumerate(targetblock.stripped_strings): # stripped: 去掉\\n,\\r,\\t, white space etc.\n",
    "            if idx == 0:\n",
    "                totalResults = int(string)\n",
    "            sentence += string + ' '\n",
    "        sentence = sentence.strip()\n",
    "        totalPages = math.ceil( totalResults / 10.0 )\n",
    "        if computePages:\n",
    "            return totalPages, totalResults, sentence\n",
    "        return totalResults, sentence\n",
    "    \n",
    "    elif task == 'crawlSchoolList':\n",
    "        targetblock = soup.find( 'div', attrs = {'class':'view-tab', 'id':'course-list'}).ul \n",
    "        headList = targetblock( 'h3')  # contains 10 li tags -> div -> h3. # soup('name') is equiv. to soup.find_all('name')\n",
    "        df = pd.DataFrame([], columns=['Link','Name','School','Location'], dtype='object')\n",
    "        for head in headList:\n",
    "            # program link, program name, name of school, school location\n",
    "            # head looks like: [<strong>Agricultural Economics</strong>, \n",
    "            # ' • ', <span>University of Hohenheim</span>, ' • Stuttgart-Hohenheim']\n",
    "            dictionary = dict()\n",
    "            if ref_ampersand == True and head.a['href']!=None:\n",
    "                dictionary['Link'] = RefAmpersand(head.a['href'])     # to see a tags attrs {keys, vals}, use: head.a.attrs\n",
    "            else:\n",
    "                dictionary['Link'] = head.a['href']\n",
    "            dictionary['Name'] = head.a.strong.string.strip()\n",
    "            dictionary['School'] = head.a.span.string.strip()\n",
    "            dictionary['Location'] = head.a.contents[-1].replace('•', '').strip()\n",
    "            # update row data into dataframe\n",
    "            df = df.append( dictionary , ignore_index = True)\n",
    "        return df\n",
    "        \n",
    "\n",
    "    elif task == 'crawlProgrammDetail':\n",
    "        targetblock = soup.find( 'div', attrs={'class':'content'} ).ul   # contains 3 li tags representing Overview, Admission, Contact tabs\n",
    "        [Overview, Admission, Contact] = targetblock('li')\n",
    "        dictionary = dict()\n",
    "\n",
    "        # dealwith 3 tabs\n",
    "        # Overview tab detail\n",
    "        for div in Overview('div'):\n",
    "            key = div.h2.string.strip()\n",
    "            if key in OverviewTabKeys and key not in dictionary:\n",
    "                str_container = []\n",
    "                for string in div.p.stripped_strings:\n",
    "                    str_container.append(string)\n",
    "                str_container = ' '.join(str_container)\n",
    "                # store value\n",
    "                dictionary[key] = str_container\n",
    "        # Admission tab detail\n",
    "        for div in Admission('div'):\n",
    "            if div.h2 != None:\n",
    "                key = div.h2.string.strip()\n",
    "            else:\n",
    "                continue\n",
    "            if key in AdmissionTabKeys and key not in dictionary:\n",
    "                str_container = []\n",
    "                for string in div.p.stripped_strings:\n",
    "                    str_container.append(string)\n",
    "                str_container = ' '.join(str_container)\n",
    "                # store value\n",
    "                dictionary[key] = str_container\n",
    "        # Contact tab detail\n",
    "        for div in Contact('div'):\n",
    "            # find correct div which contains h2 tag(s) that is International Office (AAA)\n",
    "            if div.h2 == None:\n",
    "                continue\n",
    "            key = div.h2.string.strip()\n",
    "            if key != 'International Office (AAA)':\n",
    "                continue\n",
    "            \n",
    "            for p in div('p'):\n",
    "                if p.strong != None: # omit those p tags containing strong subtags\n",
    "                    continue\n",
    "                elif p('a', href=True) != None:\n",
    "                    for a in p('a', href=True):\n",
    "                        if isMail(a['href']) and not 'AAA Mail' in dictionary:\n",
    "                            dictionary['AAA Mail'] = a['href'].replace('mailto:', '')\n",
    "                        elif isWeb(a['href']) and not 'AAA Link' in dictionary:\n",
    "                            dictionary['AAA Link'] = a['href']\n",
    "                    if key == 'International Office (AAA)' and key not in dictionary:\n",
    "                        str_container = []\n",
    "                        for string in p.stripped_strings:\n",
    "                            if re.search('Weblink.+|@.+', string): # avoid collecting mails and weblinks\n",
    "                                continue\n",
    "                            else:\n",
    "                                str_container.append(string)\n",
    "                        str_container = ' '.join(str_container)\n",
    "                        dictionary['International Office (AAA)'] = str_container\n",
    "                        break # break for loop over div('p')\n",
    "        return pd.Series(dictionary)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError( task )\n",
    "\n",
    "# helper function (True/False function required for bs4 find()/find_all() method\n",
    "def isMail(href):\n",
    "    return href and re.compile('mailto:.+').search(href)\n",
    "def isWeb(href):\n",
    "    return href and re.compile('http.+').search(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 results found totally 8 pages\n",
      "crawling and parsing page 1's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 10 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 2's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 20 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 3's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 30 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 4's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 40 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 5's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 50 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 6's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 60 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 7's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "Parsing no. 10 program details...\n",
      "data has been saved up to 70 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n",
      "crawling and parsing page 8's program list...\n",
      "Parsing no. 1 program details...\n",
      "Parsing no. 2 program details...\n",
      "Parsing no. 3 program details...\n",
      "Parsing no. 4 program details...\n",
      "Parsing no. 5 program details...\n",
      "Parsing no. 6 program details...\n",
      "Parsing no. 7 program details...\n",
      "Parsing no. 8 program details...\n",
      "Parsing no. 9 program details...\n",
      "data has been saved up to 79 in C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # params setting\n",
    "    save_path = r'C:\\Users\\rreal\\Downloads\\German_Econ_MS.csv'\n",
    "    # 'page=' (wihout number) should appear in the end of the variable source_web\n",
    "    source_web = 'https://www.daad.de/deutschland/studienangebote/studiengang/en/?a=result&q=&degree=37&subjects%5B380%5D=1&studyareas%5B380%5D=1&studyfields%5B394%5D=1&studyfields%5B390%5D=1&courselanguage=2&locations=&universities%5B1%5D=1&admissionsemester=&sort=name&page=1'\n",
    "    #totalPages = 32 # can be computed automatically\n",
    "    # if totalPages is specified, then AutoComputePages should be False.\n",
    "\n",
    "    # environ setting\n",
    "    BSparser = 'lxml'\n",
    "    AutoComputePages = True # if False, should specify totalPages\n",
    "    ref_amp = True\n",
    "    encoding ='utf-8'\n",
    "    timeSleep = 3\n",
    "    # check old file exists or not\n",
    "    if os.path.isfile(save_path):\n",
    "            ans = input( 'File already exists, rewrite it?[y/n]')\n",
    "            if ans == 'y':\n",
    "                os.remove(save_path)\n",
    "                \n",
    "    # decide wich broswer to use\n",
    "    browser = LoadBrowser( 'Chrome' )\n",
    "\n",
    "    # crawl and parser\n",
    "    # 1. crawl how many results found.\n",
    "    source_web = re.sub(r'page=\\d+', 'page=', source_web) # remove numbers after 'page='\n",
    "    codecs = Crawler( browser , source_web+str(1), sleep=timeSleep ) # return html(source code)\n",
    "    source_soup = BeautifulSoup( codecs , BSparser) # parser後，得到soup\n",
    "    # 用自己寫的Parser(crawlResultsFound), 得到重要資訊 total pages, total results, and sentence\n",
    "    totalPages, totalResults, sentence = Parser(source_soup, 'crawlResultsFound', AutoComputePages, ref_amp)\n",
    "    print(sentence, 'totally %d pages' % (totalPages))\n",
    "\n",
    "    # 2. crawl each page's program list and the program detail in it.\n",
    "    save_count = 0\n",
    "    for i in range(totalPages):\n",
    "        # 2a. crawl program list # a page usually contains 10 links\n",
    "        codecs = Crawler( browser , source_web+str(i+1), sleep=timeSleep ) # return html (source code)\n",
    "        page_soup = BeautifulSoup( codecs, BSparser )\n",
    "        print( 'crawling and parsing page %d\\'s program list...' % (i+1))\n",
    "        programListDF = Parser( page_soup, 'crawlSchoolList', AutoComputePages, ref_amp ) # get program array\n",
    "        # 2b. crawl program detail\n",
    "        col_tag= OverviewTabKeys + AdmissionTabKeys + ContactTabKeys\n",
    "        programDetailDF = pd.DataFrame([], columns= col_tag, dtype='object' )\n",
    "        for idx, [Link, Name, School, Location] in programListDF.iterrows():\n",
    "            print('Parsing no. %d program details...' % (idx+1) )\n",
    "            codecs = Crawler( browser, Link )\n",
    "            program_soup = BeautifulSoup( codecs, BSparser)\n",
    "            series = Parser( program_soup, 'crawlProgrammDetail', AutoComputePages, ref_amp)\n",
    "            programDetailDF = programDetailDF.append( series , ignore_index= True)\n",
    "        # 2c. merge dfs together\n",
    "        programDetailDF = pd.concat( [ programListDF, programDetailDF ], axis=1, ignore_index=False)\n",
    "\n",
    "        # 3. save data for every page for loop\n",
    "        programDetailDF.replace(re.compile(',|\\n'), '', inplace=True)\n",
    "        if i == 0: # the first time, so write headers into file\n",
    "            firsttime = True\n",
    "        else:\n",
    "            firsttime = False\n",
    "        programDetailDF.to_csv(save_path, sep=',', mode='a+', na_rep='', header=firsttime, index=False, encoding=encoding )\n",
    "        \n",
    "        save_count += len(programDetailDF)\n",
    "        print('data has been saved up to %d in %s' % (save_count, save_path))\n",
    "        \n",
    "    # 4. Finally quit selenium\n",
    "    browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
