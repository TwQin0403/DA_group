\documentclass{beamer} 
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage[ruled,longend]{algorithm2e}
\newtheorem*{dfn}{Definition}
\newtheorem{thm}{Theorem}[subsection]
 \renewcommand{\thethm}{\arabic{thm}}
%\newtheorem{lemma}{Lemma}
\title[Introduction]{Statistics Learning Theory:Linear Regression}
\author{}
\institute{}
\date{2019.03.09}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Linear Regression}
\begin{frame}{Linear Regression}
	\begin{enumerate}
		\item Domain set $\mathcal{X} \in R^d$
		\item Label set $\mathcal{Y}$ is the set of real number
		\item Hypothesis class
			\[\mathcal{H}_{reg} = L_d = \{x \rightarrow \langle w,x \rangle + b: w \in R^d, b \in R\}\]
		\item loss function
			\[l(h,(x,y)) = (h(x) - y)^2\]
		\item Empirical Risk function
			\[L_S(h) = \frac{1}{m} \sum^m_{i=1} (h(x_i) - y_i)^2\]
	\end{enumerate}
\end{frame}
\begin{frame}{Linear Regression: Implementation of ERM rule}
	Least square is the algorithm that solves the ERM problem for hypothesis class of linear regression predictors with respect to the squared loss. \\
	The ERM problem can be written as 
	\[arg \min_{w} L_S(h_w) = arg \min_w \frac{1}{m} \sum^m_{i=1} (\langle w,x_i \rangle - y_i)^2\]
\end{frame}
\begin{frame}{Linear Regression: Closed form solution for ERM rule}
	To solve the problem we calculate the gradient of the objective function
	\[\frac{2}{m} \sum^m_{i=1} (\langle w, x_i \rangle - y_i)x_i = 0\]
	We can rewrite the problem as the problem $Aw = b$ where
	\[A = (\sum^m_{i=1} x_i x^{\intercal}_i) \ \  \text{and} \ \ b=\sum^m_{i=1} y_i x_i \]
	If $A$ is invertible then the solution to the ERM problem is 
	\[w = A^{-1} b\]
\end{frame}
\begin{frame}{Linear Regression: ERM rule}
	If $A$ is not invertible, we still can find the solution to the system $Aw = b$ since $b$ is in the range of $A$.
	\\
	To be specific, since $A$ is symmetric we can decompose $A$ as 
	\[A = VDV^{\intercal}\]
	where $D$ is a diagonal matrix and $V$ is an orthonormal matrix. After normalizing $D$ we can obtain
	\[A^{+} = VD^{+}V^{\intercal} \ \ \text{and} \ \ \hat{w} = A^{+} b\]
	Let $v_i$ denote the $i$'th column of $V$. Then, we have 
\[A \hat{w} = A A^{+} b = VDV^{\intercal} VD^{+}V^{\intercal} b = VDD^{+} V^{\intercal} b = \sum_{i:D_{i,i} \neq 0} v_i v^{\intercal}_i b\]
\end{frame}
\begin{frame}{Consistency Theorem: The weak law of large number}
	\begin{thm}[Law of large number]
		If $z_i \in R^d$ is iid and $E|z_i| < \infty$, then
		\[\bar{z} = \frac{1}{n} \sum^n_{i=1} z_i \rightarrow^p E[z]\]
	\end{thm}
\end{frame}
\begin{frame}{Asymptotic Theorem: central limit theorem}
	\begin{thm}[Central limit theorem]
		If $z_i \in R^d$ is iid and $E|z_i|^2 < \infty$, then 
		\[\sqrt{n} (\bar{z} - \mu) = \frac{1}{\sqrt{n}} \sum^n_{i=1} (z_i - \mu) \rightarrow^d N(0,V)\]
		Where
		\[\mu = E[z_i]\]
		\[V = Var(z_i) = E[(z_i - \mu)(z_i - \mu)']\]
	\end{thm}
\end{frame}
\begin{frame}{Least square property}
	Linear projection Model:
	\[y_i = x'_i \beta + e_i  \ \ E[x_ie_i]=0\]
	OLS estimator
	\[\hat{\beta} = (\frac{1}{n} \sum^n_{i=1} x_ix'_i)^{-1} (\frac{1}{n} \sum^n_{i=1} x_iy_i)\]
	Remark: There is a strong assumption model: Regression model is to assume $E[x_i|e_i]=0$ \\
	Assumptions:
	\begin{enumerate}
		\item $\{(y_i,x'_i): i=1,\cdots n\}$ is iid
		\item $Ey^2_i< \infty$ and $E|x_i|^2 < \infty$
		\item $E[x_ix'_i]$ is positive definite
	\end{enumerate}
\end{frame}
\begin{frame}{Consistency of $\beta$}
By Cauchy-Schwarz inequality with Assumption 2: 
\[E|x_ix'_i| \leq E|x_i|^2 < \infty\]
\[E|x_i y_i|\leq \sqrt{E|x_i|^2} \sqrt{Ey^2_i} < \infty\]
The LLN implies that 
\[\frac{1}{n} \sum^n_{i=1} x_ix'_i) \rightarrow E[x_i x'_i], \ \ \frac{1}{n} \sum^n_{i=1} x_iy_i \rightarrow E[x_iy_i]\]
Therefore
\[\hat{\beta} \rightarrow^p E[x_i x'_i]^{-1} E[x_iy_i] = \beta\]
\end{frame}
\begin{frame}{Asymptotic distribution of $\hat{\beta}$}
	Since
\[\hat{\beta} = \beta + (\frac{1}{n} \sum^n_{i=1} x_ix'_i)^{-1} (\frac{1}{n} \sum^n_{i=1} x_ie_i)\]
Hence
\[\sqrt{n}(\hat{\beta} - \beta) = (\frac{1}{n} \sum^n_{i=1} x_ix'_i)^{-1}(\frac{1}{\sqrt{n}} \sum^n_{i=1} x_ie_i)\]
\end{frame}
\begin{frame}{Asymptotic distribution of $\hat{\beta}$}
	To apply CLT to $\frac{1}{\sqrt{n}} \sum^n_{i=1} x_ie_i$, we need to assume that 
	\[Ey^4_i < \infty \ \ E|x_i|^4 < \infty \]
	Under this condition we know that 
	\[E|x_ie_i|^2 = E|x_i|^2|e_i|^2 \leq \sqrt{E|x_i|^4} \sqrt{E|e_i|^4} < \infty\]
	Thus CLT implies:
	\[\frac{1}{\sqrt{n}} \sum^n_{i=1} x_i e_i \rightarrow^d N(0, \Sigma) \ \ \Sigma = E[e^2_ix_ix'_i]\]
	Since $\frac{1}{n} \sum^n_{i=1} x_ix'_i \rightarrow^p E[x_ix'_i] =Q$, continuous mapping theorem implies that  
	\[\sqrt{n}(\hat{\beta} - \beta) \rightarrow^d Q^{-1}N(0,\Sigma)\]
	\[ = N(0,Q^{-1}\Sigma Q^{-1})\]
\end{frame}
\section{Consistency Theorem}
\begin{frame}{Consistency Theorem}
	Let 
	\[Q_n(\theta) = \text{some objective function}\]
	\[\hat{\theta} = arg\max_{\theta \in \Theta} Q_n(\theta)\]
	Examples:
	\begin{enumerate}
		\item NLLS : $Q_n(\theta) = - \frac{1}{n} \sum^n_{i=1} (y_i - m(x_i,\theta))^2$
		\item ML : $Q_n(\theta) = \frac{1}{n} \sum^n_{i=1} \log f(z_i,\theta)$
		\item GMM : $Q_n(\theta) = - (\frac{1}{n} \sum^n_{i=1} g(z_i,\theta))' W (\frac{1}{n} \sum^n_{i=1}g(z_i,\theta))$
	\end{enumerate}
\end{frame}
\begin{frame}{Consistency Theorem}
	\begin{thm}[General Consistency Theorem]
		Suppose
		\begin{enumerate}
				\item $\Theta$ is compact
				\item $\sup_{\theta \in \Theta} |Q_n(\theta) - Q_{*} (\theta)| \rightarrow^p 0$ for some $Q_*:\Theta \rightarrow R$
				\item $Q_*$ is continuous in $\theta \in \Theta$
				\item $Q_*$ is uniquely maximized at $\theta_0$
		\end{enumerate}
		Then 
		\[\hat{\theta} \rightarrow^p \theta_0\]
	\end{thm}
\end{frame}
\begin{frame}{Consistency Theorem Proof}
	Pick $\varepsilon>0$, since $\hat{\theta} $ maximizes $Q_n(\theta)$
 \[Q_n(\hat{\theta}) > Q_n(\theta_0) - \frac{\varepsilon}{3}\]
 By condition 2, for any $\theta \in \Theta$
 \[|Q_n(\theta) - Q_*(\theta)| < \frac{\varepsilon}{3}\]
 with probability approaching one
\end{frame}
\begin{frame}{Consistency Theorem Proof}
	Thus with  probability approaching one
	\[Q_n(\hat{\theta}) - Q_*(\hat{\theta}) < \frac{\varepsilon}{3}\]
	\[Q_*(\theta_0) - Q_n(\theta_0) < \frac{\varepsilon}{3}\]
	Combining these inequalities
	\[Q_*(\hat{\theta}) + \frac{\varepsilon}{3} > Q_n(\hat{\theta})\]
	\[> Q_n(\theta_0) - \frac{\varepsilon}{3}\]
	\[> Q_*(\theta_0) - \frac{2 \varepsilon}{3}\]
	Therefore,
	\[Q_*(\hat{\theta}) >Q_*(\theta_0) - \varepsilon\]
\end{frame}
\begin{frame}{Consistency Theorem Proof}
	Since we want to prove that $\hat{\theta} \rightarrow^p \theta_0$, we want to show that 
	\[Pr\{\hat{\theta} \in \mathcal{N}\} \rightarrow 1\]
	for any open set $\mathcal{N} \subset \Theta$ containing $\theta_0$ \\
	Now pick any open set $\mathcal{N}$ containing $\theta_0$, since $\mathcal{N}$ is open, $\mathcal{N}^c$ is closed. By condition 1, $\Theta$ is compact, $\Theta \cap \mathcal{N}^c$ is also compact. Since $Q_*$ is continuous, Weierstrass theorem guarantees there exists $\theta_* \in \Theta \cap \mathcal{N}^c$ such that 
	\[\sup_{\Theta \cap \mathcal{N}^c} Q_*(\theta) = Q_*(\theta_*)\]
\end{frame}
\begin{frame}{Consistency Theorem Proof}
	Since $Q_*$ is uniquely maximized at $\theta_0$, we have 
	\[Q_*(\theta_0) > Q_*(\theta_*)\]
	and set
	\[\varepsilon^{'} = Q_*(\theta_0) - Q_*(\theta_*) > 0\]
	Using the previous inequality and set $\varepsilon = \varepsilon^{'}$
	\[Q_*(\hat{\theta}) > Q_*(\theta_0) - \varepsilon^{'}\]
	\[ =  Q_*(\theta_*)\]
	\[= \sup_{\Theta \cap \mathcal{N}^c} Q_*(\theta)\]
	This means that 
	\[\hat{\theta} \in \mathcal{N}\]
	with probability approaching one
\end{frame}

\begin{frame}{Consistency Theorem Remark}
	\begin{enumerate}
		\item For each application, most efforts are devoted to check Conditions 2 and 4
		\item Condition 4 is called identification condition. If $Q_*(\theta)$ is maximized at multiple points, we cannot tell where $\hat{\theta}$ converges in general
		\item Condition 2 sats that objective function $Q_n(\theta)$ uniformly converges in probability to the limit objective function $Q_*(\theta)$
		\item For condtion 2, we typically need some kind of uniform law of large numbers
	\end{enumerate}
\end{frame}

\begin{frame}{Uniform law of large number}
	\begin{thm}
		\begin{enumerate}
				\item $\Theta$ is compact
				\item $g(z,\theta)$ is almost surely continuous at each $\theta \in \Theta$
				\item There is $d(z)$ such that $|g(z,\theta) \leq d(z)$ for all $\theta \in \Theta$ and almost every $z$ and $E[d(z)] < \infty$
		\end{enumerate}
		Then 
		\[\sup_{\theta \in \Theta} |\bar{g}(\theta) - E[g(z,\theta)]| \rightarrow^p 0\]
	\end{thm}
\end{frame}

\begin{frame}{Consistency of NLLSE}
	\begin{thm}
		Suppose 
		\begin{enumerate}
				\item $\{(y_i,x^{'}_i)\}^n_{i=1}$ is iid and $E[y|x] = m(x,\theta)$ almost surely only if $\theta = \theta_0$
			\item $\Theta$ is compact
			\item $m(x,\theta)$ is almost surely continuous at each $\theta \in \Theta$
			\item $E[y^{2}] < \infty$ and $E[\sup_{\theta \in \Theta} |m(x,\theta)|^{2}] < \infty$
		\end{enumerate}
		Then 
		\[\hat{\theta} \rightarrow^p \theta_0\]
	\end{thm}
\end{frame}

\begin{frame}{Consistency of NLLSE}
	It is sufficient to check condition $1-4$ in the general theorem. Condition 1 is guaranteed by our second condition. \\
	Condition 2: $Q_*(\theta) = - E[\{y - m(x,\theta)\}^2]$, we want to show that 
	\[\sup_{\theta \in \Theta} | \frac{1}{n} \sum^n_{i=1}\{y_i - m(x_i,\theta)\}^2 - E[\{y - m(x,\theta)\}^2]| \rightarrow^p 0\]
	The above is holded by applying the ULLN. Since ULLN also guarantees continuity of $Q_*(\theta)$. Thus condition 3 is also satisfied.
\end{frame}
\begin{frame}{Consistency of NLLSE}
	It remains to check condition 4, identification of $\theta_0$. i.e.
	\[Q_*(\theta) = - E[\{y - m(x,\theta)\}^2]\]
	is uniquely maximized at $\theta_0$. Since $m(x) = E[y|x]$ solves 
	\[\min_g E[\{y - g(x)\}^2]\]
\end{frame}
\section{General Asymptotic Normality Theorem}
\begin{frame}{General Asymptotic Normality Theorem:Basic Idea}
	Now consider asymptotic distribution of extremum estimator
	\[Q_n(\theta) = \text{some objective function}\]
	\[\hat{\theta} = arg \max_{\theta \in \Theta} Q_n(\theta)\]
	Assume consistency $\hat{\theta} \rightarrow^p \theta_0$ \\
	We want to derive asymptotic normal distribution in the form of 
	\[\sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^d N(0,V)\] 
	The result can be used to construct confidence interval or to conduct hypothesis testing
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem:Basic Idea}
Suppose $Q_n(\theta)$ is continuously twice differentiable. Look at FOC for $\hat{\theta}$
\[\frac{\partial Q_n(\hat{\theta})}{\partial \theta} = 0\]
	Using the Taylor expansion, we can get 
	\[0 = \frac{\partial Q_n(\theta_0)}{\partial \theta} + \frac{\partial^2 Q_n(\tilde{\theta)}}{\partial \theta \partial \theta^{'}}(\hat{\theta} - \theta_0)\]
	where $\tilde{\theta}$ is a point on the line joining $\hat{\theta}$ and $\theta_0$. Solving for $(\hat{\theta} - \theta_0)$(if inverse exists), we get
	\[\sqrt{n} (\hat{\theta} - \theta_0) = - (\frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}})^{-1} (\sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta})\]
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem:Basic Idea}
	If 
	\[\frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} \rightarrow^p H\]
	\[\sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta} \rightarrow^d N(0,\Sigma)\]
	Then we obtain
	\[\sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^d N(0,H^{-1}\Sigma H^{-1})\]
	Typically the first condition is verified by ULLN and the second condition is verified by CLT.
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem}
	\begin{thm}
		Suppose
		\begin{enumerate}
			\item $\hat{\theta} \rightarrow^p \theta_0$ and $\theta_0 \in int\Theta$
			 \item $Q_n(\theta)$ is twice continuously differentiable in a neighborhood $\mathcal{N}$ of $\theta_0$
			\item $\sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta} \rightarrow^d N(0,\Sigma)$
			\ item There exists $H(\theta)$ that is continuous at $\theta_0$, $\sup_{\theta \in \mathcal{N}} |\frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} - H(\theta)| \rightarrow^p 0$, and $H = H(\theta_0)$ is nonsingular
		\end{enumerate}
		Then
		\[\sqrt{n}(\hat{\theta} - \theta_0) \rightarrow^d N(0,H^{-1} \Sigma H^{-1})\]
	\end{thm}
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Proof}
	Pick any convex and open set $\mathcal{N}^{'} \subset \mathcal{N}$ containing $\theta_0$, and define the indicator $\hat{I} = I\{\hat{\theta} \in \mathcal{N}^{'}\}$ \\
	Note that $\hat{I} \rightarrow^p 1$ \\
	By condition 2 and Taylor expansion we get 
	\[0 = \hat{I} \frac{\partial Q_n(\hat{\theta}}{\partial \theta}\]
		\[= \hat{I} \sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta} + \hat{I} \frac{\partial^2 Q_n(\tilde{\theta)}}{\partial \theta \partial \theta^{'}} (\hat{\theta} - \theta_0)\]
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Proof}
Since $\tilde{\theta)} \rightarrow^p \theta_0$, we have
\[|\frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} - H|\]
\[\leq |\frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} - H(\tilde{\theta})| + |H(\tilde{\theta}) - H|\]
\[\leq \sup_{\theta \in \mathcal{N}} | \frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} - H(\theta)| + |H(\tilde{\theta}) - H|\]
\[\rightarrow^p 0\]
By condition 4 and continuous mapping theorem \\
Since $H$ is nonsingular,
\[\bar{I}:= I\{\hat{\theta} \in \mathcal{N}^{'}, \frac{\partial^2 Q_n(\tilde{\theta})}{\partial \theta \partial \theta^{'}} \text{is nonsingular}\} \rightarrow^p 1\]
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Proof}
	Combining these results,
	\[0 = \bar{I} \sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta} +\bar{I} \frac{\partial^2 Q_n(\tilde{\theta)}}{\partial\theta \partial \theta^{'}} (\hat{\theta} - \theta_0)\]
	Solving for $(\hat{\theta} - \theta_0)$ we obtain
	\[\sqrt{n}(\hat{\theta} - \theta_0)\]
	\[= - \bar{I} (\frac{\partial^2 Q_n(\tilde{\theta})}{\partial \theta \partial \theta^{'}})^{-1} (\sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta}) + \{1- \bar{I}\} \sqrt{n} (\hat{\theta} - \theta_0)\]
	By condition 3 and $\bar{I} \rightarrow^p 1 $, the first term converges to $N(0, H^{-1}\Sigma H^{-1})$.
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Proof}
	It remains to show second term satisfies 
	\[\{1- \bar{I}\} \sqrt{n} (\hat{\theta} - \theta_0) \rightarrow^p 0\]
	To see this, let $Z_n = \sqrt{n} (\bar{\theta} - \theta_0)$ and pick any $\varepsilon >0$. Then
	\[Pr\{|(1-\bar{I} Z_n| > \varepsilon\}\]
		\[=Pr \{|(1-\bar{I} Z_n| > \varepsilon, \bar{I} = 1\} + Pr\{|(1-\bar{I})Z_n|>\varepsilon, \bar{I}=0\}\] 
			\[\leq Pr\{0>\varepsilon\} + Pr\{\bar{I} = 0\}\]
			\[\rightarrow 0\]
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Application to NLLSE}
	Let 
	\[Q_n(\theta) = -\frac{1}{n} \sum^n_{i=1} \{y_i - m(x_i,\theta)\}^2\]
	\[\hat{\theta} = arg \max_{\theta \in \Theta} Q_n(\theta)\]
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Application to NLLSE}
\begin{thm}
	Suppose
	\begin{enumerate}
			\item $\theta_0 \in int \Theta$
			\item $m(x,\theta)$ is twice continuously differentiable in a neighborhood $\mathcal{N}$ of $\theta_0$ with probability one
			\item $E[y^4] < \infty$ $E|\frac{\partial m(x,\theta_0)}{\partial \theta}|^4$ and 
				\[E[\sup_{\theta \in \mathcal{N}} | \frac{\partial^2 m(x,\theta)}{\partial \theta \theta^{'}}|^2 < \infty\]
				\item $H = E[\frac{\partial m(x,\theta_0)}{\partial \theta} \frac{\partial m(x,\theta_0)}{\partial \theta^{'}}]$ is nonsingular
	\end{enumerate}
	Then 
	\[\sqrt{n}(\hat{\theta} - \theta_0) \rightarrow^d N(0, H^{-1} \Sigma H^{-1})\]
	where $\Sigma = E[e^2 \frac{\partial m(x,\theta_0)}{\partial \theta} \frac{\partial m(x,\theta_0)}{\partial \theta^{'}}]$ and $e = y - m(x,\theta_0)$
\end{thm}
\end{frame}
\begin{frame}{General Asymptotic Normality Theorem: Application to NLLSE Proof}
	It is enough to check conditions for General asymptotic normality theorem. \\
	For condition 1, consistency is already verified and $\theta_0 \in int\Theta$\\
	Condition 2 is satisfied by (ii) \\
	In this case, condition 3 is verified as 
	\[\sqrt{n} \frac{\partial Q_n(\theta_0)}{\partial \theta} = \frac{1}{\sqrt{n}} \sum^n_{i=1} \frac{\partial m(x_i,\theta_0)}{\partial \theta} e_i \rightarrow^d N(0,\Sigma)\]
	where $e_i = y_i - m(x,\theta_0)$ and convergence follows from CLT with (iii)(Note that $E[y^4] <\infty \rightarrow E[e^4] < \infty$)
\end{frame} 
\section{The exercises relating to the topic}
\begin{frame}{Exercise 1:Nonlinear regression model}
	Consider the following model
	\[y_i = x^{\beta}_i + e_i \ \ E[e_i|x_i]=0 \ \ x_i >0 \ \ \text{is scalar}\]
	We aim to solve 
	\begin{enumerate}
		\item Write down the asymptotic distribution of the NLLS estimator $\hat{\beta}$.
		\item Let $\theta = \sqrt{\beta}$. Find the 95\% asymptotic confidence interval for $\theta$
		\item Let $m(c) = E[y_i|x_i=c]$ be the conditional mean at $c$. Find the 95\% asymptotic confidence interval for $m(c)$.
		\item Suppose $\sigma^2_i = E[e^2_i|x_i]$ is known. Find a better estimator than the NLLS estimator $\hat{\beta}$. Explain briefly why it is better.
	\end{enumerate}
\end{frame}
\begin{frame}{Nonlinear regression model:(a)}
	In the part, we want to apply the general theorem for asymptotic distribution, under the suitable conditions, we know that 
	\[\sqrt{n}(\hat{\beta} - \beta_0) \rightarrow^d N(0,H^{-1} \Sigma H^{-1})\]
	Where
	\[H = E[\frac{\partial m(x,\theta_0)}{\partial \theta} \frac{\partial m(x, \theta_0)}{ \partial \theta'}]\]
	\[\Sigma = E[e^2 \frac{\partial m(x,\theta_0)}{\partial \theta} \frac{\partial m(x, \theta_0)}{ \partial \theta'}]\]
	Since 
	\[\frac{\partial m(x,\beta_0)}{\partial \beta} = x^{\beta_0} \log x\]
	Therefore,
\[\sqrt{n}(\hat{\beta} - \beta_0) \rightarrow^d \]
\[ N(0, (E[x^{2\beta_0} (\log x)^2])^{-1} E[e^2[x^{2\beta_0} (\log x)^2] E[x^{2\beta_0} (\log x)^2])^{-1})\]

\end{frame}
\begin{frame}{Nonlinear regression model:(b)}
	Using the delta method, that is the Taylor expansion
	\[\theta = f(\hat{\beta}) = f(\beta_0) + \frac{\partial f(\beta)}{\partial \beta}(\tilde{\beta}) (\hat{\beta}\beta - \beta_0)\]
	where $\tilde{\beta}$ is on the line between $\hat{\beta}$ and $\beta_0$, therefore
	\[\sqrt{n}(\theta - \theta_0) = \frac{\partial f(\beta)}{\partial \beta}(\tilde{\beta}) \sqrt{n}(\hat{\beta} - \beta_0)\]
	The consistency of $\hat{\beta}$ implies $\tilde{\beta} \rightarrow^p \beta_0$, therefore
	\[\sqrt{n}(\theta - \theta_0) \rightarrow^d N(0,\frac{1}{4\beta_0} V)\]
	where
\[V = E[x^{2\beta_0} (\log x)^2])^{-1} E[e^2[x^{2\beta_0} (\log x)^2] E[x^{2\beta_0} (\log x)^2])^{-1}\]
\end{frame}
\begin{frame}{Nonlinear regression model:(b)} 
	Therefore, the 95 \% confidence interval can be constructed as
	\[ [\hat{\theta} - z_{0.025} \sqrt{\frac{\hat{V}}{4 \hat{\beta}_n}}, \hat{\theta} + z_{0.025} \sqrt{\frac{\hat{V}}{4 \hat{\beta}_n}}]\]
	where
	\[\hat{V} = (\frac{1}{n} \sum^n_{i=1} x^{2\hat{\beta}}_i(\log x_i)^2)^{-1} (\frac{1}{n} \sum^n_{i=1} \hat{e}^2_i x^{2\hat{\beta}}_i(\log x_i)^2)(\frac{1}{n} \sum^n_{i=1} x^{2\hat{\beta}}_i(\log x_i)^2)^{-1} \]
	\[\hat{e}_i = y_i - x^{\hat{\beta}}_i\]
\end{frame}
\begin{frame}{Nonlinear regression model:(c)}
Since $m(c) = c^{\beta}$ and 
\[\frac{\partial c^{\beta}}{\partial \beta} = c^{\beta} \log c\]
Using the delta method, we can get 
\[\sqrt{n}(\hat{m(c)} - m(c)) \rightarrow^d N(0,c^{2\beta} (\log c)^2 V) \]
\end{frame}
\begin{frame}{Nonlinear regression model:(d)}
To be answered in the next slide
\end{frame}
\begin{frame}{Exercise 2:linearized  estimator}
	Let $\hat{\theta} = arg\max_{\theta \in \Theta} Q_n(\theta)$ be an extremum estimator of a scalar parameter $\theta_0$, and $\hat{\theta}$ be a preliminary estimator of $\theta_0$ such that $\sqrt{n}(\bar{\theta}-\theta_0) = O_p(1)$. Consider the following 'linearized' estimator of $\theta_0$:
	\[\tilde{\theta} = \bar{\theta} - (\frac{\partial^2 Q_n(\bar{\theta})}{\partial \theta^2})^{-1} (\frac{\partial Q_n(\bar{\theta})}{\partial \theta}) \]
	Find the asympotic distribution of $\tilde{\theta}$ 
\end{frame}
\begin{frame}{Exercise 3}
	Let $f_n$ be a sequence of functions on $S \in R$ such that $\sup_{x \in S} |f_n(x) - f(x)| \rightarrow 0$ \\
	(a): show that $\sup_{x \in S} f_n(x) \rightarrow \sup_{x \in S} f(x)$ \\
	(b): show that $\inf_{x \in S} f_n(x) \rightarrow \inf_{x \in S} f(x)$
\end{frame}
\begin{frame}{Exercise 4}
	Consider the following minimum distance estimator
	\[\hat{\theta} = arg \min_{\theta \in \Theta} \{\hat{\pi} - h(\theta)\}' W \{\hat{\pi} - h(\theta)\}\]
	where $\hat{\pi} = \frac{1}{n} a(x_i)$ is a q-dimensional vector of the sample moments based on the sample $\{x_i\}^n_{i=1}, h:R^k \rightarrow R^q$ is a known function with $q \geq k$, and $W$ is a positive definite weight matrix. 
	\begin{enumerate}
		\item State assumptions and prove consistency of $\hat{\theta}$
		\item State assumptions and prove asymptotic normality of $\hat{\theta}$
	\end{enumerate}
\end{frame}
\end{document}
